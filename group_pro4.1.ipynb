{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de521196-6dcd-47e2-bc8b-e40021538793",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a7ff33d-8444-4369-8115-be15b26d9f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91498e99-b33a-4556-a5bc-fc08ae37b76b",
   "metadata": {},
   "source": [
    "# Scraping (Jia Xiang & Kesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95c9936e-9341-4d28-976d-1dfdf6f0747a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 965/965 [18:45<00:00,  1.17s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Disney's live-action Snow White is finally her...</td>\n",
       "      <td>1/10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>As always, whenever there's a negative hype ar...</td>\n",
       "      <td>1/10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Disney's latest live-action remake, Snow White...</td>\n",
       "      <td>1/10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Saw this at the cinema with my six-year-old da...</td>\n",
       "      <td>1/10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Snow White remake set out to modernize the cla...</td>\n",
       "      <td>1/10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>A truly spectacular and beautiful version of S...</td>\n",
       "      <td>10/10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>Snow White has that pure Disney magic, thanks ...</td>\n",
       "      <td>8/10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>I think that this is such a fun movie! The set...</td>\n",
       "      <td>10/10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>Magnificent, fairytale-like, mesmerizing, and ...</td>\n",
       "      <td>10/10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>I saw it and felt heartwarmed by Rachel Zegler...</td>\n",
       "      <td>10/10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>965 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Review Rating\n",
       "0    Disney's live-action Snow White is finally her...   1/10\n",
       "1    As always, whenever there's a negative hype ar...   1/10\n",
       "2    Disney's latest live-action remake, Snow White...   1/10\n",
       "3    Saw this at the cinema with my six-year-old da...   1/10\n",
       "4    Snow White remake set out to modernize the cla...   1/10\n",
       "..                                                 ...    ...\n",
       "960  A truly spectacular and beautiful version of S...  10/10\n",
       "961  Snow White has that pure Disney magic, thanks ...   8/10\n",
       "962  I think that this is such a fun movie! The set...  10/10\n",
       "963  Magnificent, fairytale-like, mesmerizing, and ...  10/10\n",
       "964  I saw it and felt heartwarmed by Rachel Zegler...  10/10\n",
       "\n",
       "[965 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "driver.get('https://www.imdb.com/title/tt6208148/reviews/?ref_=tt_urv')\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# Click All\n",
    "try:\n",
    "    all_button = WebDriverWait(driver, 5).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//button[.//span[@class='ipc-see-more__text' and text()='All']]\"))\n",
    "    )\n",
    "    driver.execute_script(\"arguments[0].click();\", all_button)\n",
    "    time.sleep(2)  \n",
    "except:\n",
    "    pass  \n",
    "\n",
    "# Scroll to load all \n",
    "count = 0\n",
    "scroll = 0\n",
    "\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)  \n",
    "    \n",
    "    mains = driver.find_elements(By.CLASS_NAME, \"ipc-list-card__content\")\n",
    "    \n",
    "    if len(mains) == count: \n",
    "        scroll += 1\n",
    "        if scroll > 5:  \n",
    "            break  \n",
    "    else:\n",
    "        scroll = 0  \n",
    "    \n",
    "    count = len(mains)\n",
    "\n",
    "# Scroll again for final check\n",
    "for _ in range(3):  \n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)  \n",
    "\n",
    "# Review and rating\n",
    "mains = driver.find_elements(By.CLASS_NAME, \"ipc-list-card__content\")\n",
    "all_reviews = []\n",
    "all_ratings = []\n",
    "\n",
    "\n",
    "for main in tqdm(mains):\n",
    "    # Review\n",
    "    ret = main.find_elements(By.CLASS_NAME, \"ipc-html-content-inner-div\")\n",
    "    review = ret[0].text.strip() if ret else \"\"  \n",
    "\n",
    "    # Rating\n",
    "    rat = main.find_elements(By.CLASS_NAME, \"sc-3e6f8aa9-4\")\n",
    "    rating = f\"{rat[0].text.strip().split('\\n')[0]}/10\" if rat else \"\"  \n",
    "\n",
    "    all_reviews.append(review)\n",
    "    all_ratings.append(rating)\n",
    "\n",
    "imdb = {\n",
    "    'Review': all_reviews,\n",
    "    'Rating': all_ratings,\n",
    "}\n",
    "\n",
    "# CSV\n",
    "df = pd.DataFrame(imdb)\n",
    "df.fillna(\"\", inplace=True)  \n",
    "df.to_csv('imdb.csv', index=False)\n",
    "driver.quit()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d034f8-a67d-45a7-afa2-7c30915ab7c5",
   "metadata": {},
   "source": [
    "# Data Cleaning and Sentiment Analysis (Thaddeus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65dd1294-5347-4a92-9cdd-7309d92d9b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Disney's live-action Snow White is finally her...</td>\n",
       "      <td>1/10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>As always, whenever there's a negative hype ar...</td>\n",
       "      <td>1/10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Disney's latest live-action remake, Snow White...</td>\n",
       "      <td>1/10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Saw this at the cinema with my six-year-old da...</td>\n",
       "      <td>1/10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Snow White remake set out to modernize the cla...</td>\n",
       "      <td>1/10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review Rating\n",
       "0  Disney's live-action Snow White is finally her...   1/10\n",
       "1  As always, whenever there's a negative hype ar...   1/10\n",
       "2  Disney's latest live-action remake, Snow White...   1/10\n",
       "3  Saw this at the cinema with my six-year-old da...   1/10\n",
       "4  Snow White remake set out to modernize the cla...   1/10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./imdb.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c64ff091-30e8-4dfc-b4e0-dd121a8d5099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.__version__\n",
    "from nltk.corpus import reuters\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8c5455c-58d9-4fd9-b774-79ffd744dfbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/thaddeusteh/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/thaddeusteh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/thaddeusteh/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/thaddeusteh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/thaddeusteh/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab') # for tokenisation\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger_eng') # POS tagging Part of speech tagging\n",
    "nltk.download('wordnet') # lemmatization\n",
    "nltk.download('vader_lexicon') # sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9c9cbf2-34a8-44d1-8e89-f343ae89de17",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preps data to be run thru sentiment analysis\n",
    "def preprocess (data):\n",
    "    words = word_tokenize(data)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    normalize_words = [word.lower() for word in words if word.isalpha()]\n",
    "    filtered_words = [word for word in normalize_words if word not in stop_words]\n",
    "    text = ' '.join(filtered_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "078c8386-f022-476d-8d06-8d4e35311fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_sentiments(df):\n",
    "    # default vals\n",
    "    df['Cleaned_Review'] = ''\n",
    "    df['VADER_Compound'] = 0.0\n",
    "    df['VADER_Positive'] = 0.0\n",
    "    df['VADER_Negative'] = 0.0\n",
    "    df['VADER_Neutral'] = 0.0\n",
    "    \n",
    "    # processing review\n",
    "    for i in range(len(df)):\n",
    "        review_text = df.loc[i, 'Review']\n",
    "        # cuci before analyze\n",
    "        clean = preprocess(review_text)\n",
    "        #print(clean_review)\n",
    "        df.at[i, 'Cleaned_Review'] = clean\n",
    "        # sentiment analysis\n",
    "        sentiment_scores = sia.polarity_scores(clean)\n",
    "        df.at[i, 'VADER_Compound'] = sentiment_scores['compound']\n",
    "        df.at[i, 'VADER_Positive'] = sentiment_scores['pos']\n",
    "        df.at[i, 'VADER_Negative'] = sentiment_scores['neg']\n",
    "        df.at[i, 'VADER_Neutral'] = sentiment_scores['neu']\n",
    "\n",
    "    file = 'imdb_sentiment_analysis.csv'\n",
    "    df.to_csv(file, index=False)\n",
    "    print(f\"done, saved to {file}.\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06f1bbc3-2750-46fd-b9aa-263f612bcd26",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m analyze_sentiments(df)\n",
      "Cell \u001b[0;32mIn[7], line 15\u001b[0m, in \u001b[0;36manalyze_sentiments\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     13\u001b[0m review_text \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReview\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# cuci before analyze\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m clean \u001b[38;5;241m=\u001b[39m preprocess(review_text)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#print(clean_review)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m df\u001b[38;5;241m.\u001b[39mat[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCleaned_Review\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m clean\n",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess\u001b[39m (data):\n\u001b[0;32m----> 3\u001b[0m     words \u001b[38;5;241m=\u001b[39m word_tokenize(data)\n\u001b[1;32m      4\u001b[0m     stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      5\u001b[0m     normalize_words \u001b[38;5;241m=\u001b[39m [word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39misalpha()]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/__init__.py:120\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    119\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1280\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;124;03m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentences_from_text(text, realign_boundaries))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1340\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1333\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1328\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m realign_boundaries:\n\u001b[1;32m   1327\u001b[0m     slices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[0;32m-> 1328\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m slices:\n\u001b[1;32m   1329\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (sentence\u001b[38;5;241m.\u001b[39mstart, sentence\u001b[38;5;241m.\u001b[39mstop)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1457\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1444\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1445\u001b[0m \u001b[38;5;124;03mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;124;03mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1454\u001b[0m \u001b[38;5;124;03m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[1;32m   1455\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1456\u001b[0m realign \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1457\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence1, sentence2 \u001b[38;5;129;01min\u001b[39;00m _pair_iter(slices):\n\u001b[1;32m   1458\u001b[0m     sentence1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(sentence1\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m realign, sentence1\u001b[38;5;241m.\u001b[39mstop)\n\u001b[1;32m   1459\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sentence2:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/punkt.py:321\u001b[0m, in \u001b[0;36m_pair_iter\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    319\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(iterator)\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 321\u001b[0m     prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1429\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1427\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_slices_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mslice\u001b[39m]:\n\u001b[1;32m   1428\u001b[0m     last_break \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1429\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m match, context \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_potential_end_contexts(text):\n\u001b[1;32m   1430\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[1;32m   1431\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(last_break, match\u001b[38;5;241m.\u001b[39mend())\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1394\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1392\u001b[0m previous_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1393\u001b[0m previous_match \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1394\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang_vars\u001b[38;5;241m.\u001b[39mperiod_context_re()\u001b[38;5;241m.\u001b[39mfinditer(text):\n\u001b[1;32m   1395\u001b[0m     \u001b[38;5;66;03m# Get the slice of the previous word\u001b[39;00m\n\u001b[1;32m   1396\u001b[0m     before_text \u001b[38;5;241m=\u001b[39m text[previous_slice\u001b[38;5;241m.\u001b[39mstop : match\u001b[38;5;241m.\u001b[39mstart()]\n\u001b[1;32m   1397\u001b[0m     index_after_last_space \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_last_whitespace_index(before_text)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object, got 'float'"
     ]
    }
   ],
   "source": [
    "df = analyze_sentiments(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f83fca3-9f74-4642-9e91-e27f46dcff3e",
   "metadata": {},
   "source": [
    "# Word Cloud (KXLeong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d20dd8b-7997-4996-8430-db1e0d7adecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d047722f-31b4-4870-b580-b1af16ce8c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(width=800,height=400,background_color='white').generate(' '.join(df[df['VADER_Compound'] < 0]['Cleaned_Review'].values))\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858bbde2-3bf2-45a9-ad3d-1c4b178552a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(width=800,height=400,background_color='white').generate(' '.join(df[df['VADER_Compound'] > 0]['Cleaned_Review'].values))\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a672f5-3c31-472d-8275-087bd5653c11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
